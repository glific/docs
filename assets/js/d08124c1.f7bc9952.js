"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8800],{7853:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Integrations/GPT integration for image recognition","title":"GPT integration for image recognition","description":"5 minutes read","source":"@site/docs/5. Integrations/GPT integration for image recognition.md","sourceDirName":"5. Integrations","slug":"/Integrations/GPT integration for image recognition","permalink":"/docs/docs/Integrations/GPT integration for image recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/glific/docs/tree/main/docs/5. Integrations/GPT integration for image recognition.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Filesearch Using OpenAI Assistants","permalink":"/docs/docs/Integrations/Filesearch Using OpenAI Assistants"},"next":{"title":"Google Maps API for reverse geo location","permalink":"/docs/docs/Integrations/Google Maps API for reverse geo location"}}');var i=n(4848),o=n(8453);const r={},a="GPT Integration for Image Recognition",d={},l=[{value:"How to Use in a Flow",id:"how-to-use-in-a-flow",level:2},{value:"Step1: Collect Image Input from the end user",id:"step1-collect-image-input-from-the-end-user",level:4},{value:"Step 2: Add a Webhook Node to Process the Image",id:"step-2-add-a-webhook-node-to-process-the-image",level:4},{value:"Step 3: Add Parameters in Function Body",id:"step-3-add-parameters-in-function-body",level:4},{value:"Step 4: Display the text response",id:"step-4-display-the-text-response",level:4},{value:"Sample Flow",id:"sample-flow",level:2}];function c(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("h3",{children:(0,i.jsx)("table",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"5 minutes read"})}),(0,i.jsx)("td",{style:{paddingLeft:"40px"},children:(0,i.jsx)("b",{children:"Level: Advanced"})}),(0,i.jsx)("td",{style:{paddingLeft:"40px"},children:(0,i.jsx)("b",{children:"Last Updated: October 2025"})})]})})}),"\n",(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"gpt-integration-for-image-recognition",children:"GPT Integration for Image Recognition"})}),"\n",(0,i.jsx)(s.p,{children:"This feature helps to recognize and understand the contents of images shared by the end users. It is useful for a variety of use cases such as:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Grading worksheets or assignments"}),"\n",(0,i.jsx)(s.li,{children:"Reading and digitizing handwritten notes"}),"\n",(0,i.jsx)(s.li,{children:"Checking skin conditions, wounds, or other health-related visuals"}),"\n",(0,i.jsx)(s.li,{children:"Reviewing the condition of water, crops, or surroundings"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"With GPT\u2019s image processing abilities, Glific makes it easy to automate tasks that involve looking at and understanding images."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"To get the best results:"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Use clear and well-lit images"}),"\n",(0,i.jsx)(s.li,{children:"Avoid blurry or low-quality photos"}),"\n",(0,i.jsx)(s.li,{children:"Make sure the main content is visible and not blocked"}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"how-to-use-in-a-flow",children:"How to Use in a Flow"}),"\n",(0,i.jsx)(s.h4,{id:"step1-collect-image-input-from-the-end-user",children:"Step1: Collect Image Input from the end user"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Create a ",(0,i.jsx)(s.code,{children:"send a message"})," node directing users to send images as their response, based on their preference."]}),"\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Wait for response"})," node, select ",(0,i.jsx)(s.code,{children:"Has image"})," as the message response type. Also, give a ",(0,i.jsx)(s.code,{children:"Result Name"}),". In the screenshot below, ",(0,i.jsx)(s.code,{children:"image"})," is used as the result name."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"500",height:"383",alt:"Screenshot 2025-09-29 at 1 29 38\u202fAM",src:"https://github.com/user-attachments/assets/4d685fb3-b126-4490-bb92-23fd9f30ed0b"}),"\n",(0,i.jsx)(s.h4,{id:"step-2-add-a-webhook-node-to-process-the-image",children:"Step 2: Add a Webhook Node to Process the Image"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Select the ",(0,i.jsx)(s.code,{children:"function"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["In the function field, enter ",(0,i.jsx)(s.code,{children:"parse_via_gpt_vision"}),", this function name is pre-defined."]}),"\n",(0,i.jsxs)(s.li,{children:["Give the webhook result name - you can use any name. In the screenshot example, it\u2019s named ",(0,i.jsx)(s.code,{children:"gptvision"}),"."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"493",height:"408",alt:"Screenshot 2025-09-29 at 1 46 28\u202fAM",src:"https://github.com/user-attachments/assets/1162f959-f923-4f5d-9214-3337456e8bda"}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)("img",{width:"504",height:"553",alt:"Screenshot 2025-09-29 at 1 46 58\u202fAM",src:"https://github.com/user-attachments/assets/b398e16c-5a6b-48ac-8c16-680c30728690"}),"\n",(0,i.jsx)(s.h4,{id:"step-3-add-parameters-in-function-body",children:"Step 3: Add Parameters in Function Body"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Click on ",(0,i.jsx)(s.code,{children:"Function Body"})," and pass the parameters as shown in the below screenshot."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"493",height:"343",alt:"Screenshot 2025-09-29 at 1 51 22\u202fAM",src:"https://github.com/user-attachments/assets/cfafbb40-04da-4321-9cf8-d457ff08835e"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"Url"}),": in this field pass the flow variable accepting the image response from the user (In the given example ",(0,i.jsx)(s.code,{children:"image"})," is the result name)."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"prompt"}),": in this field pass the prompt, or instructions you wish to convey to the AI model towards processing the image input."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"Model"}),":  in this field pass the gpt model."]}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"step-4-display-the-text-response",children:"Step 4: Display the text response"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Create a ",(0,i.jsx)(s.code,{children:"Send Message"})," node."]}),"\n",(0,i.jsxs)(s.li,{children:["Use ",(0,i.jsx)(s.code,{children:"@results.webhook_result_name.response"})," to show the text response (In the given example ",(0,i.jsx)(s.code,{children:"gptvision"})," is the webhook result name)."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"494",height:"378",alt:"Screenshot 2025-09-29 at 1 54 03\u202fAM",src:"https://github.com/user-attachments/assets/fa53b6af-a5f2-41cc-b058-91a6991ed611"}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"sample-flow",children:"Sample Flow"}),"\n",(0,i.jsxs)(s.p,{children:["Try this ",(0,i.jsx)(s.a,{href:"https://drive.google.com/file/d/1Czi9Bh7YIWGeZwpveXPu-xTGMml-h_R9/view?usp=sharing",children:"Sample Flow"})," to test the GPT Vision integration."]})]})}function h(e={}){const{wrapper:s}={...(0,o.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>r,x:()=>a});var t=n(6540);const i={},o=t.createContext(i);function r(e){const s=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(o.Provider,{value:s},e.children)}}}]);