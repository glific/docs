"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2216],{8453:(e,s,t)=>{t.d(s,{R:()=>o,x:()=>h});var n=t(6540);const i={},a=n.createContext(i);function o(e){const s=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function h(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(a.Provider,{value:s},e.children)}},9590:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>r,contentTitle:()=>h,default:()=>d,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"Integrations/Bhashini Integrations","title":"Bhashini Integrations","description":"6 minutes read","source":"@site/docs/5. Integrations/Bhashini Integrations.md","sourceDirName":"5. Integrations","slug":"/Integrations/Bhashini Integrations","permalink":"/docs/docs/Integrations/Bhashini Integrations","draft":false,"unlisted":false,"editUrl":"https://github.com/glific/docs/tree/main/docs/5. Integrations/Bhashini Integrations.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Integrations","permalink":"/docs/docs/category/integrations"},"next":{"title":"ChatGPT using OpenAI APIs","permalink":"/docs/docs/Integrations/ChatGPT using OpenAI APIs"}}');var i=t(4848),a=t(8453);const o={},h="Bhashini Integration in Glific: Speech-to-Text and Text-to-Speech",r={},c=[{value:"This integration can be especially useful in use cases such as:",id:"this-integration-can-be-especially-useful-in-use-cases-such-as",level:3},{value:"Steps to Integrate Bhashini Speech to Text in Glific Flows",id:"steps-to-integrate-bhashini-speech-to-text-in-glific-flows",level:2},{value:"Step 1: Create a <code>Send message</code> node directing users to send their responses as audio messages, based on their preference.",id:"step-1-create-a-send-message-node-directing-users-to-send-their-responses-as-audio-messages-based-on-their-preference",level:4},{value:"Step 2: In the <code>Wait for response</code> node, select <code>has audio</code> as the message response type. Also, give a Result Name. In the screenshot below, <code>speech</code> is used as the result name.",id:"step-2-in-the-wait-for-response-node-select-has-audio-as-the-message-response-type-also-give-a-result-name-in-the-screenshot-below-speech-is-used-as-the-result-name",level:4},{value:"Step 3: Add a <code>Call Webhook</code> node. This is where we integrate the Bhashini service.",id:"step-3-add-a-call-webhook-node-this-is-where-we-integrate-the-bhashini-service",level:4},{value:"Step 4: Click on <code>Function Body</code> (top right corner) and add the parameters as shown in the screenshot below",id:"step-4-click-on-function-body-top-right-corner-and-add-the-parameters-as-shown-in-the-screenshot-below",level:4},{value:"Step 5: Once the webhook is updated, you could always refer to the translated text as <code>@results.bhashini_asr.asr_response_text</code> to use it inside the flow.",id:"step-5-once-the-webhook-is-updated-you-could-always-refer-to-the-translated-text-as-resultsbhashini_asrasr_response_text-to-use-it-inside-the-flow",level:4},{value:"Steps to Integrate Bhashini Text To Speech in Glific Flows",id:"steps-to-integrate-bhashini-text-to-speech-in-glific-flows",level:2},{value:"Step 1: Create a <code>Send Message</code> node asking users to reply in text if they prefer.",id:"step-1-create-a-send-message-node-asking-users-to-reply-in-text-if-they-prefer",level:4},{value:"Step 2: In the <code>Wait for Response</code> node, select <code>has only the phrase</code> as the message response type. Also, give a Result Name. In the screenshot below, <code>result_3</code> is used as the result name.",id:"step-2-in-the-wait-for-response-node-select-has-only-the-phrase-as-the-message-response-type-also-give-a-result-name-in-the-screenshot-below-result_3-is-used-as-the-result-name",level:4},{value:"Step 3: Create a &#39;Call Webhook&#39; node.",id:"step-3-create-a-call-webhook-node",level:4},{value:"Step 4: Click Function Body (top right corner) and add the parameters as shown in the screenshot below.",id:"step-4-click-function-body-top-right-corner-and-add-the-parameters-as-shown-in-the-screenshot-below",level:4},{value:"Step 5: Create a <code>send Message</code> node and paste the variable.",id:"step-5-create-a-send-message-node-and-paste-the-variable",level:4},{value:"Step 6: To get the translated text out, create another send message node, and call the <code>@results.bhashini_tts.translated_text</code>.",id:"step-6-to-get-the-translated-text-out-create-another-send-message-node-and-call-the-resultsbhashini_ttstranslated_text",level:4},{value:"Using OpenAI Speech Engine for Text-to-Speech",id:"using-openai-speech-engine-for-text-to-speech",level:2},{value:"How to configure:",id:"how-to-configure",level:3},{value:"Blogs",id:"blogs",level:3},{value:"Video of Showcase",id:"video-of-showcase",level:3}];function l(e){const s={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("h3",{children:(0,i.jsx)("table",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"6 minutes read"})}),(0,i.jsx)("td",{style:{paddingLeft:40},children:(0,i.jsx)("b",{children:"Level: Advanced"})}),(0,i.jsx)("td",{style:{paddingLeft:40},children:(0,i.jsx)("b",{children:"Last Updated: Septemper 2025"})})]})})}),"\n",(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"bhashini-integration-in-glific-speech-to-text-and-text-to-speech",children:"Bhashini Integration in Glific: Speech-to-Text and Text-to-Speech"})}),"\n",(0,i.jsx)(s.p,{children:"Bhashini is an initiative by the Government of India aimed at making digital services and the internet accessible in all Indian languages.\nThe integration of Bhashini into Glific enables NGOs and organizations to offer real-time translation and transliteration capabilities in various Indian languages, ensuring effective communication with end users in their preferred languages."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"this-integration-can-be-especially-useful-in-use-cases-such-as",children:"This integration can be especially useful in use cases such as:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Translating chatbot content for multilingual campaigns."}),"\n",(0,i.jsx)(s.li,{children:"Enabling users to respond in regional languages."}),"\n",(0,i.jsx)(s.li,{children:"Transliteration helps convert text from one script to another, for example: writing Hindi words using English letters."}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Bhashini specializes in Indic language translation and transliteration, supporting a wide range of languages and dialects. You can learn more about the platform ",(0,i.jsx)(s.a,{href:"https://bhashini.gov.in",children:"here"})]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Note: Bhashini integration is still in an experimental phase. While it enables powerful multilingual speech capabilities, users may occasionally notice variations in response quality or stability. As development continues, these aspects are expected to improve over time."})}),"\n",(0,i.jsx)(s.h2,{id:"steps-to-integrate-bhashini-speech-to-text-in-glific-flows",children:"Steps to Integrate Bhashini Speech to Text in Glific Flows"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"Speech-to-Text (STT)"})," function in Glific can be used to convert user-recorded audio messages into text. This is especially helpful when users prefer speaking over typing, or in cases where typing in local languages is difficult."]}),"\n",(0,i.jsxs)(s.h4,{id:"step-1-create-a-send-message-node-directing-users-to-send-their-responses-as-audio-messages-based-on-their-preference",children:["Step 1: Create a ",(0,i.jsx)(s.code,{children:"Send message"})," node directing users to send their responses as audio messages, based on their preference."]}),"\n",(0,i.jsxs)(s.h4,{id:"step-2-in-the-wait-for-response-node-select-has-audio-as-the-message-response-type-also-give-a-result-name-in-the-screenshot-below-speech-is-used-as-the-result-name",children:["Step 2: In the ",(0,i.jsx)(s.code,{children:"Wait for response"})," node, select ",(0,i.jsx)(s.code,{children:"has audio"})," as the message response type. Also, give a Result Name. In the screenshot below, ",(0,i.jsx)(s.code,{children:"speech"})," is used as the result name."]}),"\n",(0,i.jsx)("img",{width:"509",height:"390",alt:"Screenshot 2025-08-10 at 12 10 35\u202fAM",src:"https://github.com/user-attachments/assets/9a273e08-6860-41ca-b693-9c22d952a81a"}),"\n",(0,i.jsxs)(s.h4,{id:"step-3-add-a-call-webhook-node-this-is-where-we-integrate-the-bhashini-service",children:["Step 3: Add a ",(0,i.jsx)(s.code,{children:"Call Webhook"})," node. This is where we integrate the Bhashini service."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Select the ",(0,i.jsx)(s.code,{children:"Function"})," from the dropdown."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["In the ",(0,i.jsx)(s.code,{children:"Function"})," field, enter ",(0,i.jsx)(s.code,{children:"speech_to_text_with_bhasini"}),". The function name is pre-defined, you should always use the function ",(0,i.jsx)(s.code,{children:"speech_to_text_with_bhasini"})," to call the Bhashini API for converting audio to text."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Give the webhook result name - you can use any name. In the screenshot example, it\u2019s named ",(0,i.jsx)(s.code,{children:"bhashini_asr"}),"."]}),"\n",(0,i.jsx)("img",{width:"517",height:"431",alt:"Screenshot 2025-08-10 at 12 13 00\u202fAM",src:"https://github.com/user-attachments/assets/42e354bf-ec76-4d59-a1c0-1b610b45fcc3"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.h4,{id:"step-4-click-on-function-body-top-right-corner-and-add-the-parameters-as-shown-in-the-screenshot-below",children:["Step 4: Click on ",(0,i.jsx)(s.code,{children:"Function Body"})," (top right corner) and add the parameters as shown in the screenshot below"]}),"\n",(0,i.jsx)("img",{width:"557",height:"388",alt:"Screenshot 2025-08-10 at 12 14 30\u202fAM",src:"https://github.com/user-attachments/assets/0130ebd7-cdf6-42e0-8505-a0c92821e990"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"speech"})," : It should be updated with the result name given for the audio file captured. In this example, the variable is named ",(0,i.jsx)(s.code,{children:"speech"})," (Step 2), hence the value is ",(0,i.jsx)(s.code,{children:"@results.speech.input"})," (If the audio note captured was saved as ",(0,i.jsx)(s.code,{children:"query"}),", then the value will be ",(0,i.jsx)(s.code,{children:"@results.query.input"}),")"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"contact"})," : Keep the value as given in the screenshot below - ",(0,i.jsx)(s.code,{children:"@contact"})]}),"\n"]}),"\n",(0,i.jsxs)(s.h4,{id:"step-5-once-the-webhook-is-updated-you-could-always-refer-to-the-translated-text-as-resultsbhashini_asrasr_response_text-to-use-it-inside-the-flow",children:["Step 5: Once the webhook is updated, you could always refer to the translated text as ",(0,i.jsx)(s.code,{children:"@results.bhashini_asr.asr_response_text"})," to use it inside the flow."]}),"\n",(0,i.jsxs)(s.p,{children:["Add a ",(0,i.jsx)(s.code,{children:"Send Message"})," node and paste this variable to show the converted text to the user."]}),"\n",(0,i.jsx)("img",{width:"761",height:"606",alt:"Screenshot 2025-09-25 at 12 46 56\u202fAM",src:"https://github.com/user-attachments/assets/ba852558-826e-40ff-9790-0697dc720a1a"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.a,{href:"https://drive.google.com/file/d/1F5oJGRxE7G6RgpyG77q2srqnikUZMDab/view?usp=sharing",children:"Sample Flow"})," Click on the Sample Flow link to import it and explore how it works."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"steps-to-integrate-bhashini-text-to-speech-in-glific-flows",children:"Steps to Integrate Bhashini Text To Speech in Glific Flows"}),"\n",(0,i.jsx)(s.p,{children:"Text-to-Speech (TTS) function in Glific can be used to generate a voice note for any text message, whether it's typed by the end user or written by NGO staff. This allows organizations to make information more accessible, especially for end users who prefer audio over text."}),"\n",(0,i.jsx)("img",{width:"593",height:"673",alt:"Screenshot 2025-09-25 at 12 51 19\u202fAM",src:"https://github.com/user-attachments/assets/4094f6fe-e832-4f59-a5d4-197591820369"}),"\n",(0,i.jsxs)(s.h4,{id:"step-1-create-a-send-message-node-asking-users-to-reply-in-text-if-they-prefer",children:["Step 1: Create a ",(0,i.jsx)(s.code,{children:"Send Message"})," node asking users to reply in text if they prefer."]}),"\n",(0,i.jsxs)(s.h4,{id:"step-2-in-the-wait-for-response-node-select-has-only-the-phrase-as-the-message-response-type-also-give-a-result-name-in-the-screenshot-below-result_3-is-used-as-the-result-name",children:["Step 2: In the ",(0,i.jsx)(s.code,{children:"Wait for Response"})," node, select ",(0,i.jsx)(s.code,{children:"has only the phrase"})," as the message response type. Also, give a Result Name. In the screenshot below, ",(0,i.jsx)(s.code,{children:"result_3"})," is used as the result name."]}),"\n",(0,i.jsx)("img",{width:"620",height:"432",alt:"Screenshot 2025-08-10 at 12 27 34\u202fAM",src:"https://github.com/user-attachments/assets/2c2da4dc-39e9-43c0-9112-14b8138999cf"}),"\n",(0,i.jsx)(s.h4,{id:"step-3-create-a-call-webhook-node",children:"Step 3: Create a 'Call Webhook' node."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Select the ",(0,i.jsx)(s.code,{children:"Function"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Function"})," field, enter ",(0,i.jsx)(s.code,{children:"nmt_tts_with_bhasini"})," The function name is pre-defined, you should always use the function ",(0,i.jsx)(s.code,{children:"nmt_tts_with_bhasini"})," to call the Bhashini API for converting text to audio."]}),"\n",(0,i.jsxs)(s.li,{children:["Give the webhook result name - you can use any name. In the screenshot example, it\u2019s named ",(0,i.jsx)(s.code,{children:"bhashini_tts"}),"."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"586",height:"493",alt:"Screenshot 2025-09-25 at 12 54 19\u202fAM",src:"https://github.com/user-attachments/assets/e43df145-63d8-43c0-aa87-e476798ea484"}),"\n",(0,i.jsx)(s.h4,{id:"step-4-click-function-body-top-right-corner-and-add-the-parameters-as-shown-in-the-screenshot-below",children:"Step 4: Click Function Body (top right corner) and add the parameters as shown in the screenshot below."}),"\n",(0,i.jsx)("img",{width:"616",height:"431",alt:"Screenshot 2025-08-10 at 12 34 03\u202fAM",src:"https://github.com/user-attachments/assets/ce4a15ea-8eb7-4861-996a-03159e2bca96"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"text"})," : It should be updated with the result name given for the response/query provided by the user."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"Source_language"})," : The original language of the text"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"target_language"})," : The language in which the voice note will be generated"]}),"\n",(0,i.jsxs)(s.li,{children:["If translation is not needed, keep both ",(0,i.jsx)(s.code,{children:"Source_language"})," and ",(0,i.jsx)(s.code,{children:"target_language"})," the same."]}),"\n",(0,i.jsxs)(s.li,{children:["Supported Target Languages: ",(0,i.jsx)(s.code,{children:'"tamil" "kannada" "malayalam" "telugu" "assamese" "gujarati" "bengali" "punjabi" "marathi" "urdu" "spanish" "english" "hindi"'})]}),"\n"]}),"\n",(0,i.jsxs)(s.h4,{id:"step-5-create-a-send-message-node-and-paste-the-variable",children:["Step 5: Create a ",(0,i.jsx)(s.code,{children:"send Message"})," node and paste the variable."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"@results.bhashini_tts.media_url"})," for the voice input. ",(0,i.jsx)(s.code,{children:"Bhashini_tts"})," is the webhook result name used in the given example."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Go to ",(0,i.jsx)(s.code,{children:"Attachments"})," in the ",(0,i.jsx)(s.code,{children:"Send Message"})," node"]}),"\n",(0,i.jsxs)(s.li,{children:["Select ",(0,i.jsx)(s.code,{children:"Expression"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["Use the following expression: ",(0,i.jsx)(s.code,{children:"@results.bhashini_tts.media_url"})]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"588",height:"239",alt:"Screenshot 2025-09-25 at 12 56 50\u202fAM",src:"https://github.com/user-attachments/assets/ebf00923-4da8-4944-ab5f-b929b52ce5fc"}),"\n",(0,i.jsxs)(s.p,{children:["Please note: In order to get the voice notes as outputs, the Glific instance must be linked to the Google Cloud Storage for your organization. This is to facilitate storage of the voice notes generated by Bhashini as a result of the webhook call. To set up Google Cloud Storage ",(0,i.jsx)(s.a,{href:"https://glific.github.io/docs/docs/Onboarding/GCS%20Setup/Google%20Cloud%20Storage%20Setup/",children:"click here"})]}),"\n",(0,i.jsxs)(s.h4,{id:"step-6-to-get-the-translated-text-out-create-another-send-message-node-and-call-the-resultsbhashini_ttstranslated_text",children:["Step 6: To get the translated text out, create another send message node, and call the ",(0,i.jsx)(s.code,{children:"@results.bhashini_tts.translated_text"}),"."]}),"\n",(0,i.jsx)("img",{width:"586",height:"451",alt:"Screenshot 2025-09-25 at 12 57 47\u202fAM",src:"https://github.com/user-attachments/assets/bcbd8b0f-1b80-4d6c-8c2b-9753b2802d8e"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.a,{href:"https://drive.google.com/file/d/1WCOLQMF-OgLVR7PNHXbggMSeDXMJbui7/view",children:"Sample Flow"})," Click on the Sample Flow link to import it and explore how it works."]}),"\n",(0,i.jsx)(s.h2,{id:"using-openai-speech-engine-for-text-to-speech",children:"Using OpenAI Speech Engine for Text-to-Speech"}),"\n",(0,i.jsx)(s.p,{children:"Apart from Bhashini, the OpenAI speech engine can also be used to generate text-to-speech (TTS) responses. Since we are also experimenting with Bhashini, the response quality may sometimes be inconsistent or unreliable in a few languages.This is another alternative, users can try both options to see which gives better results for their audience and language preferences."}),"\n",(0,i.jsx)(s.h3,{id:"how-to-configure",children:"How to configure:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Function Body"}),", set the speech engine to ",(0,i.jsx)(s.code,{children:"open-ai"}),"."]}),"\n",(0,i.jsx)(s.li,{children:"Keep the remaining steps the same as those mentioned in the Speech-to-Text section above."}),"\n"]}),"\n",(0,i.jsx)("img",{width:"590",height:"412",alt:"Screenshot 2025-09-25 at 1 00 36\u202fAM",src:"https://github.com/user-attachments/assets/1111db5f-137d-4825-8ca9-d3e6a00fe53f"}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"blogs",children:"Blogs"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://glific.org/the-importance-of-mother-language-in-the-indian-development-sector/",children:"Blog Link"})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"video-of-showcase",children:"Video of Showcase"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=zS83U9OJJzk",children:"Video Link"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.em,{children:"Watch from 25 minute mark to watch the Bhashini integration part"})})]})}function d(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);