"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1874],{7577:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"Integrations/Filesearch Using OpenAI Assistants","title":"Filesearch Using OpenAI Assistants","description":"6 minutes read","source":"@site/docs/5. Integrations/Filesearch Using OpenAI Assistants.md","sourceDirName":"5. Integrations","slug":"/Integrations/Filesearch Using OpenAI Assistants","permalink":"/docs/docs/Integrations/Filesearch Using OpenAI Assistants","draft":false,"unlisted":false,"editUrl":"https://github.com/glific/docs/tree/main/docs/5. Integrations/Filesearch Using OpenAI Assistants.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ChatGPT using OpenAI APIs","permalink":"/docs/docs/Integrations/ChatGPT using OpenAI APIs"},"next":{"title":"GPT integration for image recognition","permalink":"/docs/docs/Integrations/GPT integration for image recognition"}}');var i=t(4848),a=t(8453);const o={},r="File Search Using OpenAI Assistants",l={},h=[{value:"Use this when:",id:"use-this-when",level:3},{value:"This document guides you through three main parts:",id:"this-document-guides-you-through-three-main-parts",level:3},{value:"How to Create an OpenAI Assistant in Glific",id:"how-to-create-an-openai-assistant-in-glific",level:2},{value:"Step 1: Create a new AI Assistant",id:"step-1-create-a-new-ai-assistant",level:4},{value:"Step 2: Fill in the Assistant Details",id:"step-2-fill-in-the-assistant-details",level:4},{value:"Step 3: Save Your Assistant",id:"step-3-save-your-assistant",level:4},{value:"Step 4: Copy the Assistant ID",id:"step-4-copy-the-assistant-id",level:4},{value:"Using the OpenAI Assistant in Floweditor",id:"using-the-openai-assistant-in-floweditor",level:2},{value:"Handling text inputs and outputs",id:"handling-text-inputs-and-outputs",level:2},{value:"Step 1: Get User Question",id:"step-1-get-user-question",level:4},{value:"Step 2: Add a  Call Webhook node. This is where we integrate the OpenAI Assistant.",id:"step-2-add-a--call-webhook-node-this-is-where-we-integrate-the-openai-assistant",level:4},{value:"Step 3: Click on Function Body (top right corner) and pass the following parameter",id:"step-3-click-on-function-body-top-right-corner-and-pass-the-following-parameter",level:4},{value:"Step 4:  Display the Assistant&#39;s response",id:"step-4--display-the-assistants-response",level:4},{value:"Conversational Memory",id:"conversational-memory",level:3},{value:"Step 5: Add <code>thread_id</code> in the next Webhook call",id:"step-5-add-thread_id-in-the-next-webhook-call",level:4},{value:"Handling Voice Inputs and Responses",id:"handling-voice-inputs-and-responses",level:2},{value:"Step 1: Capture end user\u2019s voice input",id:"step-1-capture-end-users-voice-input",level:4},{value:"Step 2: Create Call a Webhook node",id:"step-2-create-call-a-webhook-node",level:4},{value:"Step 3: Click on Function Body (top right corner) and pass the following parameter",id:"step-3-click-on-function-body-top-right-corner-and-pass-the-following-parameter-1",level:4},{value:"Step 4: Display the text response",id:"step-4-display-the-text-response",level:4},{value:"Step 5: Send the voice note response",id:"step-5-send-the-voice-note-response",level:4},{value:"Pricing",id:"pricing",level:2},{value:"Video of Showcase",id:"video-of-showcase",level:2}];function d(e){const s={a:"a",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("h3",{children:(0,i.jsx)("table",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"6 minutes read"})}),(0,i.jsx)("td",{style:{paddingLeft:40},children:(0,i.jsx)("b",{children:" Level: Advanced"})}),(0,i.jsx)("td",{style:{paddingLeft:40},children:(0,i.jsx)("b",{children:"Last Updated: August 2025"})})]})})}),"\n",(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"file-search-using-openai-assistants",children:"File Search Using OpenAI Assistants"})}),"\n",(0,i.jsxs)(s.p,{children:["Glific\u2019s File Search using OpenAI Assistant enables users to upload documents and get AI-generated answers to user questions. The system uses a method called ",(0,i.jsx)(s.strong,{children:"Retrieval Augmented Generation (RAG)"}),", where the assistant searches through your files to give accurate, helpful responses, including answers to follow-up questions."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"use-this-when",children:"Use this when:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Users want to ask questions based on your PDFs, reports, or manuals."}),"\n",(0,i.jsx)(s.li,{children:"There is a need to build an automated knowledge assistant for your organisation."}),"\n",(0,i.jsx)(s.li,{children:"Help users get instant responses."}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"this-document-guides-you-through-three-main-parts",children:"This document guides you through three main parts:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Creating an OpenAI Assistant"}),"\n",(0,i.jsx)(s.li,{children:"Using the Assistant in your Flows (including handling follow-up questions)"}),"\n",(0,i.jsx)(s.li,{children:"Handling Voice Inputs and Responses"}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"how-to-create-an-openai-assistant-in-glific",children:"How to Create an OpenAI Assistant in Glific"}),"\n",(0,i.jsx)(s.h4,{id:"step-1-create-a-new-ai-assistant",children:"Step 1: Create a new AI Assistant"}),"\n",(0,i.jsxs)(s.p,{children:["Click on ",(0,i.jsx)(s.code,{children:"AI Assistant"})," from the left sidebar, then select ",(0,i.jsx)(s.code,{children:"Create Assistant"})," to generate a blank assistant."]}),"\n",(0,i.jsx)("img",{width:"633",height:"367",alt:"Screenshot 2025-08-09 at 12 35 30\u202fAM",src:"https://github.com/user-attachments/assets/6f33287c-1dd1-45af-8e57-f58c36b81cff"}),"\n",(0,i.jsx)(s.h4,{id:"step-2-fill-in-the-assistant-details",children:"Step 2: Fill in the Assistant Details"}),"\n",(0,i.jsx)(s.p,{children:"Define the following parameters:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Choose the most relevant model from the first drop down."}),"\n",(0,i.jsx)(s.li,{children:"Provide a name to the assistant."}),"\n",(0,i.jsxs)(s.li,{children:["Provide a system prompt in the ",(0,i.jsx)(s.code,{children:"Instructions"})," field.",(0,i.jsx)(s.br,{}),"\n",(0,i.jsxs)(s.em,{children:[(0,i.jsx)(s.a,{href:"https://glific.org/a-simple-guide-to-using-large-language-models/#prompt",children:"Click Here"})," to read more on prompt engineering."]})]}),"\n",(0,i.jsxs)(s.li,{children:["Files (PDF, DOCX, etc.) can be uploaded by clicking on ",(0,i.jsx)(s.code,{children:"Manage Files"}),". These files will be utilized by the assistant to generate responses.",(0,i.jsx)(s.br,{}),"\n",(0,i.jsxs)(s.em,{children:[(0,i.jsx)(s.a,{href:"https://platform.openai.com/docs/assistants/tools/file-search/supported-files#supported-files",children:"Click Here"})," to know the supported file formats by the OpenAI APIs."]})]}),"\n",(0,i.jsxs)(s.li,{children:["Set the ",(0,i.jsx)(s.code,{children:"Temperature"})," (between 0 to 2). A higher value increases creativity/randomness.",(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.em,{children:"Recommended: keep temperature at 0."})]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Note:"})," The quality of the bot\u2019s response depends on the prompt. Give appropriate prompts based on your use case."]}),"\n",(0,i.jsx)(s.h4,{id:"step-3-save-your-assistant",children:"Step 3: Save Your Assistant"}),"\n",(0,i.jsxs)(s.p,{children:["Once the files are added, click on ",(0,i.jsx)(s.code,{children:"Add"}),". This completes the Assistant setup.",(0,i.jsx)(s.br,{}),"\n","Click on the ",(0,i.jsx)(s.code,{children:"Save"})," button after making any changes."]}),"\n",(0,i.jsx)("img",{width:"656",height:"405",alt:"Screenshot 2025-08-09 at 12 43 48\u202fAM",src:"https://github.com/user-attachments/assets/f3e31a36-1c8e-4c2f-91d5-954c22fcb7d9"}),"\n",(0,i.jsx)(s.h4,{id:"step-4-copy-the-assistant-id",children:"Step 4: Copy the Assistant ID"}),"\n",(0,i.jsxs)(s.p,{children:["Once created, copy the ",(0,i.jsx)(s.code,{children:"Assistant ID"})," shown below the assistant name.",(0,i.jsx)(s.br,{}),"\n","This ID will be used in the webhook nodes in the flow editor."]}),"\n",(0,i.jsx)("img",{width:"357",height:"408",alt:"Screenshot 2025-08-09 at 12 46 25\u202fAM",src:"https://github.com/user-attachments/assets/8d7556f0-827a-44ea-a2e5-881f2b61c2e1"}),"\n",(0,i.jsx)(s.h2,{id:"using-the-openai-assistant-in-floweditor",children:"Using the OpenAI Assistant in Floweditor"}),"\n",(0,i.jsx)(s.p,{children:"The following sections explain how to use an assistant to answer questions or create conversations."}),"\n",(0,i.jsx)(s.h2,{id:"handling-text-inputs-and-outputs",children:"Handling text inputs and outputs"}),"\n",(0,i.jsx)(s.p,{children:"This section explains how to:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Use the ",(0,i.jsx)(s.code,{children:"filesearch-gpt"})," webhook function to pass a user\u2019s question to the OpenAI Assistant."]}),"\n",(0,i.jsx)(s.li,{children:"Receive the assistant\u2019s response."}),"\n",(0,i.jsx)(s.li,{children:"Handle follow-up questions using conversational memory."}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"step-1-get-user-question",children:"Step 1: Get User Question"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Create a flow where the user sends a question as text input."}),"\n",(0,i.jsxs)(s.li,{children:["Add ",(0,i.jsx)(s.code,{children:"Send Message"})," node and receive user question as text."]}),"\n",(0,i.jsx)(s.li,{children:"This question will be passed to the assistant for a response."}),"\n",(0,i.jsxs)(s.li,{children:["Provide a ",(0,i.jsx)(s.code,{children:"Result Name"})," for the `Wait for Response node."]}),"\n",(0,i.jsxs)(s.li,{children:["In the example below, the result name is set as ",(0,i.jsx)(s.code,{children:"question"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.em,{children:"Screenshot of example flow set up is given below"})}),"\n",(0,i.jsx)("img",{width:"633",height:"501",alt:"Screenshot 2025-08-09 at 12 51 47\u202fAM",src:"https://github.com/user-attachments/assets/6cb60c7a-6b75-4c96-8b78-88fa3318a8c2"}),"\n",(0,i.jsx)(s.h4,{id:"step-2-add-a--call-webhook-node-this-is-where-we-integrate-the-openai-assistant",children:"Step 2: Add a  Call Webhook node. This is where we integrate the OpenAI Assistant."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Select the ",(0,i.jsx)(s.code,{children:"Function"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Function"}),"  field, enter ",(0,i.jsx)(s.code,{children:"filesearch-gpt"}),", this function name is pre-defined."]}),"\n",(0,i.jsxs)(s.li,{children:["Give the webhook ",(0,i.jsx)(s.code,{children:"Result Name"})," - you can use any name. In the screenshot example, it\u2019s named as ",(0,i.jsx)(s.code,{children:"gptresponse"}),"."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"633",height:"525",alt:"Screenshot 2025-08-09 at 1 17 28\u202fAM",src:"https://github.com/user-attachments/assets/eead58e1-a32a-4aa9-9fb0-4e4de2fc16e5"}),"\n",(0,i.jsx)(s.h4,{id:"step-3-click-on-function-body-top-right-corner-and-pass-the-following-parameter",children:"Step 3: Click on Function Body (top right corner) and pass the following parameter"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.code,{children:'{ "question": "@results.question", "assistant_id": "asst_xxxxx", "remove_citation":true }'})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["In ",(0,i.jsx)(s.code,{children:"question"})," parameter enter the flow variable containing the question asked by the user. In the given example ",(0,i.jsx)(s.code,{children:"question"})," is the ",(0,i.jsx)(s.code,{children:"result name"}),", hence provided ",(0,i.jsx)(s.code,{children:"@result.question"})," in the question parameter."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["In ",(0,i.jsx)(s.code,{children:"assistant_id"}),' enter the assistant id obtained in step 4 of "How to Create an OpenAI Assistant in Glific"']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["In ",(0,i.jsx)(s.code,{children:"remove_citation"})," enter ",(0,i.jsx)(s.code,{children:"true"})," to prevent cryptic citation marks from showing up in the response."]}),"\n",(0,i.jsx)("img",{width:"633",height:"525",alt:"Screenshot 2025-08-09 at 1 17 28\u202fAM",src:"https://github.com/user-attachments/assets/2b4d9c60-3a4e-4ba6-b63b-d221acf621ad"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"step-4--display-the-assistants-response",children:"Step 4:  Display the Assistant's response"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["Once the Webhook is updated, add a ",(0,i.jsx)(s.code,{children:"Send Message"})," node and enter ",(0,i.jsx)(s.code,{children:"@results.gptresponse.message"})," variable to receive the AI response."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:["In the given example  ",(0,i.jsx)(s.code,{children:"gptresponse"})," is the ",(0,i.jsx)(s.code,{children:"result name"})," (refer step2). If ",(0,i.jsx)(s.code,{children:"AI_response"})," was the result name, the variable would be ",(0,i.jsx)(s.code,{children:"@results.AI_response.message"}),"."]}),"\n",(0,i.jsx)("img",{width:"643",height:"498",alt:"Screenshot 2025-08-09 at 1 28 11\u202fAM",src:"https://github.com/user-attachments/assets/6c4d143e-a1b9-4523-a684-8eae3e0e0e97"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.a,{href:"https://drive.google.com/file/d/1-RcFXdEpeuqlb27RWNRpWNdZ9PNZzBSz/view",children:"Sample Flow"})," Click on the Sample Flow link to import it and explore how it works."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"conversational-memory",children:"Conversational Memory"}),"\n",(0,i.jsx)(s.p,{children:"When a user asks a follow-up question, the assistant uses thread ID to remember the earlier conversation. This helps it give better answers by understanding the context of what was already asked."}),"\n",(0,i.jsxs)(s.h4,{id:"step-5-add-thread_id-in-the-next-webhook-call",children:["Step 5: Add ",(0,i.jsx)(s.code,{children:"thread_id"})," in the next Webhook call"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["To answer the subsequent questions based on the context of a question already asked, in the next webhook call, an additional parameter called ",(0,i.jsx)(s.code,{children:"thread_id"})," needs to be passed."]}),"\n",(0,i.jsxs)(s.li,{children:["This parameter should be set to the value ",(0,i.jsx)(s.code,{children:"@results.previouswebhookname.thread_id"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["In the example shown, the previous webhook result name is gptresponse. So the thread ID should be referenced as - ",(0,i.jsx)(s.code,{children:"@results.gptresponse.thread_id"}),"."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"644",height:"452",alt:"Screenshot 2025-08-09 at 1 33 23\u202fAM",src:"https://github.com/user-attachments/assets/fae67969-664d-4e56-89ea-5ba6adc74f6c"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["In question parameter enter the flow variable containing the follow up question asked by the user. In the given example ",(0,i.jsx)(s.code,{children:"result_5"})," is the result name, hence provided ",(0,i.jsx)(s.code,{children:"@results.result_5"})," in the question parameter."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"624",height:"489",alt:"Screenshot 2025-08-09 at 1 34 26\u202fAM",src:"https://github.com/user-attachments/assets/bd95bbfb-18e1-46d0-915a-fad38782b5d4"}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"handling-voice-inputs-and-responses",children:"Handling Voice Inputs and Responses"}),"\n",(0,i.jsxs)(s.p,{children:["Some beneficiaries may find it easier to talk instead of typing. This is helpful for people who are not comfortable reading or writing. With voice input, beneficiaries can send voice notes to ask questions and get answers as both text and voice.\nThis section explains how to use the ",(0,i.jsx)(s.code,{children:"voice-filesearch-gpt"})," webhook function in Glific flows to take a user\u2019s voice note as input and return both text and voice note responses in the desired language."]}),"\n",(0,i.jsx)(s.h4,{id:"step-1-capture-end-users-voice-input",children:"Step 1: Capture end user\u2019s voice input"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Create a ",(0,i.jsx)(s.code,{children:"Send Message"})," node directing users to send their responses as audio messages, based on their preference."]}),"\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Wait for Response"})," node, select has audio as the message response type. Also, give a result name. In the screenshot below, ",(0,i.jsx)(s.code,{children:"audio_query"})," is used as the result name."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"606",height:"463",alt:"Screenshot 2025-08-09 at 1 38 18\u202fAM",src:"https://github.com/user-attachments/assets/5805f7fd-f13f-4298-b9e3-bf63f235574c"}),"\n",(0,i.jsx)(s.h4,{id:"step-2-create-call-a-webhook-node",children:"Step 2: Create Call a Webhook node"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Select ",(0,i.jsx)(s.code,{children:"Function"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["In the ",(0,i.jsx)(s.code,{children:"Function"})," field, enter ",(0,i.jsx)(s.code,{children:"voice-filesearch-gpt"})," , this function name is pre-defined."]}),"\n",(0,i.jsxs)(s.li,{children:["Give the webhook result name - you can use any name. In the screenshot example, it\u2019s named ",(0,i.jsx)(s.code,{children:"gpt_voice"}),"."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"615",height:"512",alt:"Screenshot 2025-08-09 at 1 40 02\u202fAM",src:"https://github.com/user-attachments/assets/c5375afe-9e30-4264-b4ab-0d0e9e83cd7b"}),"\n",(0,i.jsx)("img",{width:"625",height:"528",alt:"Screenshot 2025-08-09 at 1 40 28\u202fAM",src:"https://github.com/user-attachments/assets/3ccb9af7-1a01-4fbb-9358-7a842c8c7960"}),"\n",(0,i.jsx)(s.h4,{id:"step-3-click-on-function-body-top-right-corner-and-pass-the-following-parameter-1",children:"Step 3: Click on Function Body (top right corner) and pass the following parameter"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-{",children:'  "contact": "@contact",\n  "speech": "@results.audio_query.input",\n  "assistant_id": "asst_xxxxxxxx",\n  "remove_citation": true,\n  "source_language": "@contact.language",\n  "target_language": "hindi"\n}\n'})}),"\n",(0,i.jsx)("img",{width:"617",height:"430",alt:"Screenshot 2025-08-09 at 1 42 16\u202fAM",src:"https://github.com/user-attachments/assets/2f0e99f1-eb86-404b-9dc6-b7a97da944ce"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"speech"})," is the result name which is storing the voice note sent by the user."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"assistant_id"}),' is the assistant id obtained in step 4 of "How to Create an OpenAI Assistant in Glific.']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"source_langauge"})," is the expected language of the user."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"target_language"})," is the language that the response voice note needs to be in."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"remove_citation"})," pass as ",(0,i.jsx)(s.code,{children:"true"})," to avoid citation marks to be part of the response voice note."]}),"\n"]}),"\n",(0,i.jsx)(s.h4,{id:"step-4-display-the-text-response",children:"Step 4: Display the text response"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Create a ",(0,i.jsx)(s.code,{children:"Send Message node"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["Use ",(0,i.jsx)(s.code,{children:"@results.webhook_result-name.translated_text"})," to show the text response."]}),"\n",(0,i.jsxs)(s.li,{children:["In the given example ",(0,i.jsx)(s.code,{children:"gpt_voice"})," is the webhook result name."]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"622",height:"473",alt:"Screenshot 2025-08-09 at 1 45 38\u202fAM",src:"https://github.com/user-attachments/assets/bd42793c-f4a4-4326-814b-aea9ac7e7924"}),"\n",(0,i.jsx)(s.h4,{id:"step-5-send-the-voice-note-response",children:"Step 5: Send the voice note response"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["In a new ",(0,i.jsx)(s.code,{children:"Send Message"})," node, go to ",(0,i.jsx)(s.code,{children:"Attachments"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["Choose ",(0,i.jsx)(s.code,{children:"Expression"})," from the dropdown."]}),"\n",(0,i.jsxs)(s.li,{children:["Use  ",(0,i.jsx)(s.code,{children:"@results.gpt_voice.media_url"})," (gpt_voice is the result name of webhook node)"]}),"\n"]}),"\n",(0,i.jsx)("img",{width:"617",height:"252",alt:"Screenshot 2025-08-09 at 1 46 58\u202fAM",src:"https://github.com/user-attachments/assets/547eed06-5697-4379-95d2-0221dc6093e6"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.a,{href:"https://drive.google.com/file/d/1jFwNoGiUCqalbC8K-slUnI5tt3fGyVRc/view",children:"Sample Flow"})," Click on the Sample Flow link to import it and explore how it works."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"pricing",children:"Pricing"}),"\n",(0,i.jsx)(s.p,{children:"NGOs can use AI features in Glific without any additional cost for inferencing. Glific is supported by OpenAI to help more NGOs experiment, pilot, and run programs using large language models (LLMs), enabling them to scale their impact without being limited by cost. Additionally NGOs can use up to $100 worth of credits until August 2026."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"video-of-showcase",children:"Video of Showcase"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://www.youtube.com/watch?v=J_sFgOUFFOA",children:"Video Link"})})]})}function c(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,s,t)=>{t.d(s,{R:()=>o,x:()=>r});var n=t(6540);const i={},a=n.createContext(i);function o(e){const s=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(a.Provider,{value:s},e.children)}}}]);